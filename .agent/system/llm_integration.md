# LLM Integration Architecture

This document describes how Visual Scaffolding integrates with Large Language Models to enable natural language flow generation.

## Overview

The system uses LLMs with tool calling capabilities to translate user requests into flow operations (adding nodes, creating edges, etc.). It implements a robust retry mechanism to handle failures and uses conversation history for context awareness.

## LLM Providers

- **Primary**: Groq (`groq-sdk`, model: `openai/gpt-oss-120b`)
- **Fallback**: Cerebras (`@cerebras/cerebras_cloud_sdk`, model: `gpt-oss-120b`)
- Automatic failover if Groq fails


## System Prompt

Located in `server/llm/llmService.js`. Defines the LLM's role as a UI helper for React Flow and specifies the expected response format (XML with `<thinking>` and `<response>` tags containing JSON tool calls).

## Context Building

Every LLM request includes (`buildLLMContext` in `llmService.js`):

### 1. Conversation History
- Last 6 interaction pairs (12 messages)
- Fetched from `conversation_history` table
- Provides context about what user has asked before
- Helps LLM avoid repeating mistakes

### 2. Current Flow State
- All nodes with: id, label, description, position
- All edges with: id, source, target, label
- JSON stringified for readability

### 3. Visual Settings Snapshot
- Background color/gradient
- Global node colors and per-node overrides
- Node dimensions (defaults + overrides), zoom level, dagre spacing, fit view padding
- Gives the LLM awareness of current presentation before issuing adjustments

### 4. Available Tools
- Tool definitions with names, descriptions, parameters
- OpenAI function calling format

### 5. User Message
- The current request from user
- Can be initial request or retry message

**Example Context Message:**
```
Current Flow State:
{
  "nodes": [
    {
      "id": "login",
      "type": "default",
      "position": {"x": 0, "y": 0},
      "data": {
        "label": "Login",
        "description": "User auth"
      }
    }
  ],
  "edges": []
}

Available Tools:
addNode: Creates a new node. If parentNodeId is provided...
Parameters: {
  "type": "object",
  "properties": {...}
}

User Request: add a home page after login
```

## Tool Definitions

Located in `server/llm/tools.js`. Defined in OpenAI function calling format with name, description, and parameter schemas.

### Available Tools

1. **addNode** - Create node with optional parent connection
   - Smart ID generation from label
   - Auto-edge creation if `parentNodeId` provided
   - Optional edge labeling

2. **updateNode** - Modify existing node
   - Update label, description, or position

3. **deleteNode** - Remove node and connected edges

4. **addEdge** - Connect two existing nodes

5. **updateEdge** - Change edge label

6. **deleteEdge** - Remove connection

7. **undo** - Revert last change

8. **redo** - Reapply undone change

9. **changeVisuals** - Update background or node colors (global or per-node overrides)

10. **changeDimensions** - Adjust node sizing, zoom level, or dagre spacing by ¬±10%

**Key Features:**
- `addNode` accepts label as `parentNodeId` (auto-matches to node)
- Sanitization: labels converted to IDs (e.g., "My Node" ‚Üí "my_node") to help LLMs perform multiple node creation and connections in fewer calls.
- Validation: all params validated before execution

## Response Parsing

LLM responses are parsed in `parseToolCalls()`. Extracts `<thinking>` and `<response>` tags, strips JSON comments, parses as JSON array, and converts to internal format. Parse errors are returned to frontend without retrying.

## Tool Execution

Located in `server/tools/executor.js`. Tools are executed sequentially with state passed between them. Flow edits are batched in a single DB write with automatic undo snapshot, while visual/dimension changes persist through `saveVisualSettings()`.

### Result Format
```javascript
{
  success: true,
  nodeId: "home",        // For addNode
  edgeId: "e123",        // For addEdge
  updatedFlow: {...},    // Modified flow state
}

// Or on error:
{
  success: false,
  error: "Node login not found"
}
```

## Error Recovery System

Handles cases where tool execution fails (e.g., referencing non-existent nodes). Implements automatic retry loop (max 3 iterations) where failed tool calls trigger a detailed retry message sent back to the LLM with current flow state and error details.

### Retry Message Format

Generated by `buildRetryMessage()` - shows success/failure status for each tool, lists all available node IDs, and provides instructions for retry.

```
Previous tool execution results:

‚úÖ addNode({"label":"Login"}) ‚Üí Success!
‚ùå addNode({"label":"Home","parentNodeId":"logn"}) ‚Üí Failed
   Error: Parent node logn not found

üìä Current Flow State:
  Available Nodes (1 total):
    ‚Ä¢ "Login" (ID: "login") - User authentication

üîß Instructions:
Please retry the failed operations using the correct node IDs shown above.
```

### Iteration Tracking

Frontend receives `iterations` field indicating how many attempts were made (e.g., `iterations: 2` means succeeded on 2nd attempt).

## Conversation Management

All messages saved to `conversation_history` table. Only last 6 interaction pairs sent to LLM for context. Clear history via `DELETE /api/conversation/history` (not exposed in UI).

## Test Coverage

- `tests/llm/llmService.test.js`
- `tests/llm/llmParsing-edgecases.test.js`
- `tests/integration/message-retry.test.js`
- `tests/toolExecution.test.js`

## Manual Testing

```bash
curl -X POST http://localhost:3001/api/conversation/message \
  -H "Content-Type: application/json" \
  -d '{"message": "create a login flow with 3 steps"}'
```
